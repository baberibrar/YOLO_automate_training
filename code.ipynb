{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0EP-o5Dar_Y",
    "outputId": "29402edc-9de4-44b1-e220-a85ab3261eae"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import ndarray, ndimage\n",
    "!pip install split-folders\n",
    "import splitfolders\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZBku47Rav0y"
   },
   "outputs": [],
   "source": [
    "!unzip -q /content/automatic_training.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNoLVs94bo0Q"
   },
   "outputs": [],
   "source": [
    "os.remove('/content/automatic_training.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcvlXU8Bar_f"
   },
   "outputs": [],
   "source": [
    "\n",
    "def json_from_dir(source_dir):\n",
    "    all_jsons = glob.glob(os.path.join(source_dir,\"*.json\"))\n",
    "    if all_jsons:\n",
    "        all_jsons = {p:os.path.getmtime(p) for p in all_jsons}\n",
    "        return sorted(all_jsons.items())[0][0]\n",
    "    return None\n",
    "\n",
    "\n",
    "def rotate(image_array: ndarray, angle):\n",
    "    return ndimage.rotate(image_array, angle, reshape=True, order=0)\n",
    "\n",
    "\n",
    "def augment_image(img, angle, im_name):\n",
    "    augmented = rotate(img, angle)\n",
    "    name_parts = im_name.split(\".\")\n",
    "    name, ext = name_parts[0], name_parts[-1]\n",
    "    aug_im_name = name + \"_\" + str(angle) + \".\" + ext\n",
    "    return augmented, aug_im_name\n",
    "\n",
    "\n",
    "def get_and_adjust_rotation_matrix(angle, cx, cy, h, w):\n",
    "    M = cv2.getRotationMatrix2D((cx, cy), angle, 1.0)\n",
    "    cosang = np.abs(M[0, 0])\n",
    "    sinang = np.abs(M[0, 1])\n",
    "    nW = int((h * sinang) + (w * cosang))\n",
    "    nH = int((h * cosang) + (w * sinang))\n",
    "    M[0, 2] += (nW / 2) - cx\n",
    "    M[1, 2] += (nH / 2) - cy\n",
    "    return M\n",
    "\n",
    "\n",
    "def rotatePolygon(corners, M, w, h, ln=8):\n",
    "    corners = corners.reshape(-1,2)\n",
    "    corners = np.hstack((corners, np.ones((corners.shape[0],1), dtype = type(corners[0][0]))))\n",
    "    calculated = np.dot(M,corners.T).T\n",
    "    calculated = calculated.reshape(-1,ln)\n",
    "    if len(np.where(calculated[:, range(0,ln,2)] > w)) == 0 or len(np.where(calculated[:, range(1,ln,2)] > h)) == 0:\n",
    "        return None\n",
    "    return calculated\n",
    "\n",
    "\n",
    "def transform_regions(regions, rotation_matrix, w, h):\n",
    "    r_list = []\n",
    "    for region in regions:\n",
    "        label = list(region[\"region_attributes\"][\"class\"])[0]\n",
    "        x1 = region[\"shape_attributes\"][\"x\"]\n",
    "        y1 = region[\"shape_attributes\"][\"y\"]\n",
    "        width = region[\"shape_attributes\"][\"width\"]\n",
    "        height = region[\"shape_attributes\"][\"height\"]\n",
    "        x2 = width + x1\n",
    "        y2 = height + y1\n",
    "        nl = np.array([x1,y1,x2,y1,x2,y2,x1,y2])\n",
    "        p = rotatePolygon(np.array([nl]), rotation_matrix, w, h, ln=len(nl))[0]\n",
    "        r_list += [\n",
    "            {\n",
    "                'shape_attributes': {\n",
    "                    'name': 'polygon',\n",
    "                    \"all_points_x\": [p[i] for i in range(0,len(p),2)],\n",
    "                    \"all_points_y\": [p[i] for i in range(1,len(p),2)]\n",
    "                },\n",
    "                'region_attributes': {\n",
    "                    'class': {\n",
    "                        label: True\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    return r_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IvsSE3blar_k",
    "outputId": "aa5c31ad-e410-4c9f-d0a4-5d501364f070"
   },
   "outputs": [],
   "source": [
    "root_dir = \"/content/automatic_training/images\"\n",
    "dest_dir = root_dir.rstrip(\"/\") + \"_augmented\"\n",
    "\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.mkdir(dest_dir)\n",
    "\n",
    "json_path = \"/content/automatic_training/images/annotation.json\"\n",
    "with open(json_path) as f:\n",
    "    annotations_data = json.load(f)\n",
    "\n",
    "new_annotations_data = {\n",
    "    \"_via_settings\": copy.deepcopy(annotations_data[\"_via_settings\"]),\n",
    "    \"_via_img_metadata\": {},\n",
    "    \"_via_attributes\": copy.deepcopy(annotations_data[\"_via_attributes\"])\n",
    "}\n",
    "\n",
    "count = 0\n",
    "for im_key in annotations_data[\"_via_img_metadata\"]:\n",
    "    count += 1\n",
    "    print(\"processing image \"+str(count)+\"/\"+str(len(annotations_data[\"_via_img_metadata\"]))+\" ...\")\n",
    "    im_name = annotations_data[\"_via_img_metadata\"][im_key][\"filename\"]\n",
    "    im_path = os.path.join(root_dir,im_name)\n",
    "    if os.path.exists(im_path):\n",
    "        img = cv2.imread(im_path)\n",
    "        if len(img.shape) is 2:\n",
    "            img = gray2rgb(img)\n",
    "        img = img[:,:,:3]\n",
    "        height, width = img.shape[0],img.shape[1]\n",
    "        cx = width//2\n",
    "        cy = height//2\n",
    "        for angle in range(0, 360, 2):\n",
    "            augmented, aug_im_name = augment_image(img, angle, im_name)\n",
    "            aug_im_path = os.path.join(dest_dir, aug_im_name)\n",
    "            cv2.imwrite(aug_im_path, augmented)\n",
    "            rotation_matrix = get_and_adjust_rotation_matrix(angle, cx, cy, height, width)\n",
    "            aug_size = os.path.getsize(aug_im_path)\n",
    "            aug_im_key = aug_im_name + str(aug_size)\n",
    "            new_annotations_data[\"_via_img_metadata\"][aug_im_key] = {\n",
    "                \"filename\": aug_im_name,\n",
    "                \"size\": aug_size,\n",
    "                \"regions\": transform_regions(annotations_data[\"_via_img_metadata\"][im_key][\"regions\"], rotation_matrix, width, height),\n",
    "                \"file_attributes\": {}\n",
    "            }\n",
    "\n",
    "aug_json_path = os.path.join(dest_dir,\"annoatations.json\")\n",
    "with open(aug_json_path,\"w\") as f:\n",
    "    json.dump(new_annotations_data,f)\n",
    "    print(\"Done saving in \"+dest_dir)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xez8apvPar_i"
   },
   "outputs": [],
   "source": [
    "def get_images():\n",
    "    images=os.listdir('/content/automatic_training/images_augmented')\n",
    "#     print(images)\n",
    "    temp={}\n",
    "    for i in images:\n",
    "        try:\n",
    "            temp[i] = cv2.imread(f'/content/automatic_training/images_augmented/{i}')\n",
    "        except Exception as E:\n",
    "            pass\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XiHsf4har_j"
   },
   "outputs": [],
   "source": [
    "images_data = get_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoQdqTB2ar_h"
   },
   "outputs": [],
   "source": [
    "data=open('/content/automatic_training/images_augmented/annoatations.json').read()\n",
    "data=json.loads(data)\n",
    "# print(data)\n",
    "def get_final_box(corners):\n",
    "    #main_array = np.array(calculated) # converting to numpy array\n",
    "    x_ = corners[:4]\n",
    "    y_ = corners[4:]\n",
    "    \n",
    "    xmin = np.min(x_).reshape(-1,1)\n",
    "    ymin = np.min(y_).reshape(-1,1)\n",
    "    xmax = np.max(x_).reshape(-1,1)\n",
    "    ymax = np.max(y_).reshape(-1,1)\n",
    "    \n",
    "    ymax=ymax-ymin\n",
    "    xmax=xmax-xmin\n",
    "    final = np.hstack((xmin, ymin, xmax, ymax))\n",
    "    \n",
    "    return final\n",
    "for x in list(data['_via_img_metadata'].values()):\n",
    "    filename = x['filename']\n",
    "    for region in x['regions']:\n",
    "        all_points_x = region['shape_attributes']['all_points_x']\n",
    "        all_points_y = region['shape_attributes']['all_points_y']\n",
    "        class_name = list(region['region_attributes']['class'].keys())[0]\n",
    "        #print(filename, all_points_x, all_points_y,class_name)\n",
    "        all_points_x.extend(all_points_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NceqhFDar_l"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/automatic_training/images_augmented')\n",
    "if os.path.exists('annoatations.json'):\n",
    "    def normalize_for_yolo(xmin, ymin, w, h, w_img, h_img):\n",
    "        xcenter = (xmin + w/2) / h_img\n",
    "        ycenter = (ymin + h/2) / w_img\n",
    "        w = w / h_img\n",
    "        h = h / w_img\n",
    "        return xcenter,ycenter,w,h\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    class_dt = {}\n",
    "    \n",
    "    os.chdir('/content/automatic_training')\n",
    "    if not os.path.exists('yolo_annotation'):\n",
    "        os.mkdir('yolo_annotation')\n",
    "\n",
    "    for x in list(data['_via_img_metadata'].values()):\n",
    "        filename = x['filename']\n",
    "        u_filename = filename.split('.')[0]\n",
    "        width, height = (images_data[filename].shape)[:2]\n",
    "        f = open(f'yolo_annotation/{u_filename}.txt', \"w\")\n",
    "\n",
    "        for region in x['regions']:\n",
    "            all_points_x = region['shape_attributes']['all_points_x']\n",
    "            all_points_y = region['shape_attributes']['all_points_y']\n",
    "            class_name = list(region['region_attributes']['class'].keys())[0]\n",
    "            if class_dt.__contains__(class_name)==False:\n",
    "                class_dt[class_name] = count\n",
    "                count += 1\n",
    "\n",
    "    #         print(filename, all_points_x, all_points_y,class_name)\n",
    "            all_points_x.extend(all_points_y)\n",
    "            four_points = get_final_box(all_points_x)\n",
    "            x, y, w, h = four_points[0] \n",
    "            x, y, w, h = normalize_for_yolo(x, y, w, h, width, height)\n",
    "\n",
    "            f.write(f\"{class_dt[class_name]} {x} {y} {w} {h}\\n\")\n",
    "    #         break\n",
    "        f.close()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uQsvZrEar_m"
   },
   "outputs": [],
   "source": [
    "def  making_directories():\n",
    "    if not os.path.exists('split'):\n",
    "        os.mkdir('split')\n",
    "    os.chdir('split')\n",
    "    if not os.path.exists('images_augmented'):\n",
    "        os.mkdir('images_augmented')\n",
    "    if not os.path.exists('yolo_annotation'):\n",
    "        os.mkdir('yolo_annotation')\n",
    "def copying_images():\n",
    "    os.chdir(\"/content/automatic_training\")\n",
    "    for files in glob.glob('images_augmented/*.jpeg'):\n",
    "        shutil.copyfile(f'{files}', f'split/{files}')\n",
    "def copying_text():\n",
    "    for files in glob.glob('yolo_annotation/*.txt'):\n",
    "        shutil.copyfile(f'{files}', f'split/{files}')\n",
    "def splitting_train_test():\n",
    "    splitfolders.ratio('split', output=\"output\", seed=1337, ratio=(.8, 0.2))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0gzpaCBZar_n",
    "outputId": "02102a9d-e23e-46e1-8bb6-52bea702459e"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/content/automatic_training\")\n",
    "if len(os.listdir('images_augmented'))-1==len(os.listdir(\"yolo_annotation\")):\n",
    "    making_directories()\n",
    "    copying_images()\n",
    "    copying_text()\n",
    "    splitting_train_test()\n",
    "    !git clone https://github.com/ultralytics/yolov5.git\n",
    "    %cd yolov5/\n",
    "    !pip install -r requirements.txt\n",
    "    import yaml\n",
    "\n",
    "    with open('/content/automatic_training/yolov5/data/coco128.yaml',encoding=\"utf8\") as f:\n",
    "        doc = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    doc['train']='/content/automatic_training/output/train'\n",
    "    doc['val']='/content/automatic_training/output/val'\n",
    "    doc['nc']=2\n",
    "    doc['names']=['cat','dog']\n",
    "    del doc['path']\n",
    "    del doc['test']\n",
    "    del doc['download']\n",
    "    with open('/content/automatic_training/yolov5/data/coco128.yaml', 'w') as f:\n",
    "        yaml.dump(doc, f)\n",
    "\n",
    "\n",
    "    !python train.py --img 600 --batch 20 --epochs 5 --data coco128.yaml --weights yolov5s.pt --nosave --cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chxUjzUrar_p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "If4xoKd_ar_p"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "augment (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
